{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8432ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,ENGLISH_STOP_WORDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723ac2e",
   "metadata": {},
   "source": [
    "# Part 1 Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc3668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.'\n",
      "  'business']\n",
      " ['german business confidence slides german business confidence fell in february knocking hopes of a speedy recovery in europe s largest economy.  munich-based research institute ifo said that its confidence index fell to 95.5 in february from 97.5 in january  its first decline in three months. the study found that the outlook in both the manufacturing and retail sectors had worsened. observers had been hoping that a more confident business sector would signal that economic activity was picking up.   we re surprised that the ifo index has taken such a knock   said dz bank economist bernd weidensteiner.  the main reason is probably that the domestic economy is still weak  particularly in the retail trade.  economy and labour minister wolfgang clement called the dip in february s ifo confidence figure  a very mild decline . he said that despite the retreat  the index remained at a relatively high level and that he expected  a modest economic upswing  to continue.  germany s economy grew 1.6% last year after shrinking in 2003. however  the economy contracted by 0.2% during the last three months of 2004  mainly due to the reluctance of consumers to spend. latest indications are that growth is still proving elusive and ifo president hans-werner sinn said any improvement in german domestic demand was sluggish. exports had kept things going during the first half of 2004  but demand for exports was then hit as the value of the euro hit record levels making german products less competitive overseas. on top of that  the unemployment rate has been stuck at close to 10% and manufacturing firms  including daimlerchrysler  siemens and volkswagen  have been negotiating with unions over cost cutting measures. analysts said that the ifo figures and germany s continuing problems may delay an interest rate rise by the european central bank. eurozone interest rates are at 2%  but comments from senior officials have recently focused on the threat of inflation  prompting fears that interest rates may rise.'\n",
      "  'business']\n",
      " ['bbc poll indicates economic gloom citizens in a majority of nations surveyed in a bbc world service poll believe the world economy is worsening.  most respondents also said their national economy was getting worse. but when asked about their own family s financial outlook  a majority in 14 countries said they were positive about the future. almost 23 000 people in 22 countries were questioned for the poll  which was mostly conducted before the asian tsunami disaster. the poll found that a majority or plurality of people in 13 countries believed the economy was going downhill  compared with respondents in nine countries who believed it was improving. those surveyed in three countries were split. in percentage terms  an average of 44% of respondents in each country said the world economy was getting worse  compared to 34% who said it was improving. similarly  48% were pessimistic about their national economy  while 41% were optimistic. and 47% saw their family s economic conditions improving  as against 36% who said they were getting worse.  the poll of 22 953 people was conducted by the international polling firm globescan  together with the program on international policy attitudes (pipa) at the university of maryland.  while the world economy has picked up from difficult times just a few years ago  people seem to not have fully absorbed this development  though they are personally experiencing its effects   said pipa director steven kull.  people around the world are saying:  i m ok  but the world isn t .  there may be a perception that war  terrorism and religious and political divisions are making the world a worse place  even though that has not so far been reflected in global economic performance  says the bbc s elizabeth blunt.  the countries where people were most optimistic  both for the world and for their own families  were two fast-growing developing economies  china and india  followed by indonesia. china has seen two decades of blistering economic growth  which has led to wealth creation on a huge scale  says the bbc s louisa lim in beijing. but the results also may reflect the untrammelled confidence of people who are subject to endless government propaganda about their country s rosy economic future  our correspondent says. south korea was the most pessimistic  while respondents in italy and mexico were also quite gloomy. the bbc s david willey in rome says one reason for that result is the changeover from the lira to the euro in 2001  which is widely viewed as the biggest reason why their wages and salaries are worth less than they used to be. the philippines was among the most upbeat countries on prospects for respondents  families  but one of the most pessimistic about the world economy. pipa conducted the poll from 15 november 2004 to 3 january 2005 across 22 countries in face-to-face or telephone interviews. the interviews took place between 15 november 2004 and 5 january 2005. the margin of error is between 2.5 and 4 points  depending on the country. in eight of the countries  the sample was limited to major metropolitan areas.'\n",
      "  'business']\n",
      " ...\n",
      " ['weak dollar hits reuters revenues at media group reuters slipped 11% during 2004  mainly due to the weakness of the dollar  the group said.  the company said it was optimistic about growth even as revenues slipped 11% from £3.24bn ($6.13bn) in 2003 to £2.89bn in 2004. reuters beat profit forecasts  posting a 52% rise in profits for the year to £198m from the £130m seen a year earlier. reuters also beat its savings target for 2004  delivering £234m of cuts. under its three-year fast forward turnaround plan it had aimed to save £220m during the 12 months to 31 december.  reuters also managed to slow a decline in underlying revenues to 5.4% from 10.2% in 2003 and cut its debt back to £160m from £610m a year earlier. the news and financial data seller said the year had begun well  adding it expected  further gradual improvement  in the second quarter of the year after good january sales. it added it was planning to deliver a further £105m of savings over the coming year - but said it expects to be hit with an £80m restructuring charge to pay for the cost of moving from fleet street to new headquarters in london at canary wharf.  improving customer relationships  more competitive products and continued strong cost discipline position us well for 2005   chief executive tom glocer said  adding the company was beginning  to look beyond recovery to growth .'\n",
      "  'business']\n",
      " ['apple ipod family expands market apple has expanded its ipod family with the release of its next generation of the digital music players.  its latest challenges to the growing digital music gadget market include an ipod mini model which can hold 6gb compared to a previous 4gb. the company  which hopes to keep its dominant place in the digital music market  also said the gold coloured version of the mini would be dropped. a 30gb version has also been added to the ipod photo family. the latest models have a longer battery life and their prices have been cut by an average of £40. the original ipod took an early lead in the digital music player market thanks to its large storage capacity and simple design.  during 2004 about 25 million portable players were sold  10 million of which were apple ipods. but analysts agree that the success is also down to its integration with the itunes online store  which has given the company a 70% share of the legal download music market. mike mcguire  a research director at analyst gartner  told the bbc news website that apple had done a good job in  sealing off the market from competition  so far.  they have created a very seamless package which i think is the idea of the product - the design  function and the software are very impressive   he said. he added that the threat from others was always present  however.  creative  other microsoft-partnered devices  real  sony and so on  are ratcheting up the marketing message and advertising   he said. creative was very upbeat about how many of its creative zen players it had shipped by the end of last year  he said. its second-generation models  like the creative zen micro photo  is due out in the summer. it will have 5gb of memory on board.  digital music players are now the gadget of choice among young americans  according to recent research by the pew internet and american life project. one in 10 us adults - 22 million people - now owns a digital music player of some sort. sales of legally downloaded songs also rose more than tenfold in 2004  according to the record industry  with 200 million tracks bought online in the us and europe in 12 months. the ifpi industry body said that the popularity of portable music players was behind the growth. analysts say that the ease of use and growth of music services available on the net will continue to drive the trend towards portable music players.  people are also starting to use them in novel ways. some are combining automatic syncing functions many of them have with other net functions to automatically distribute diy radio shows  called podcasts. but 2005 will also see more competition from mobile phone operators who are keen to offer streaming services on much more powerful and sophisticated handsets. according to mr mcguire  research suggests that people like the idea of building up huge libraries of music  which they can do with high-capacity storage devices  like ipods and creative zens. mobiles do not yet have this capacity though  and there are issues about the ease of portability of mobile music. mr mcguire said apple was ensuring it kept a foot in the mobile music door with its recent deal with motorola to produce a version of itunes for motorola phones.'\n",
      "  'tech']\n",
      " ['santy worm makes unwelcome visit thousands of website bulletin boards have been defaced by a virus that used google to spread across the net.  the santy worm first appeared on 20 december and within 24 hours had successfully hit more than 40 000 websites. the malicious program exploits a vulnerability in the widely used phpbb software. santy s spread has now been stopped after google began blocking infected sites searching for new victims.  the worm replaces chat forums with a webpage announcing that the site had been defaced by the malicious program. soon after being infected  sites hit by the worm started randomly searching for other websites running the vulnerable phpbb software. once google started blocking these search queries the rate of infection tailed off sharply. a message sent to finnish security firm f-secure by google s security team said:  while a seven hour response for something like this is not outrageous  we think we can and should do better.   we will be reviewing our procedures to improve our response time in the future to similar problems   the google team said. security firms estimate that about 1m websites run their discussion groups and forums with the open source phpbb program. the worst of the attack now seems to be over as a search conducted on the morning of the 22 december produced only 1 440 hits for sites showing the text used in the defacement message. people using the sites hit by santy will not be affected by the worm. santy is not the first malicious program to use google to help it spread. in july a variant of the mydoom virus slowed down searches on google as the program flooded the search site with queries looking for new e-mail addresses to send itself to.'\n",
      "  'tech']]\n",
      "business --> ('the', 7133) ('to', 3306) ('of', 2864) ('in', 2821) ('and', 2161) ('said', 1100) ('is', 1072) ('that', 1052) ('for', 1045) ('it', 1011) ('on', 906) ('has', 835) ('its', 736) ('by', 718) ('with', 612) ('at', 609) ('as', 605) ('was', 596) ('from', 588) ('be', 573) ('have', 554) ('are', 538) ('us', 522) ('will', 520) ('year', 456) ('which', 404) ('mr', 393) ('but', 391) ('an', 385) ('had', 343) ('this', 322) ('would', 309) ('been', 307) ('up', 306) ('not', 302) ('more', 298) ('he', 287) ('market', 284) ('were', 282) ('also', 279) ('than', 275) ('new', 273) ('their', 265) ('firm', 261) ('growth', 257) ('company', 253) ('last', 236) ('economy', 233) ('about', 228) ('after', 221)\n",
      "\n",
      "tech --> ('the', 7498) ('to', 4149) ('of', 3425) ('and', 3017) ('in', 2316) ('that', 1676) ('is', 1597) ('it', 1465) ('for', 1299) ('on', 1111) ('said', 1064) ('be', 1057) ('are', 994) ('as', 929) ('will', 797) ('with', 787) ('have', 731) ('by', 727) ('has', 686) ('was', 653) ('people', 647) ('they', 630) ('more', 624) ('not', 535) ('at', 533) ('but', 527) ('which', 514) ('from', 500) ('he', 484) ('can', 480) ('this', 474) ('or', 452) ('their', 422) ('up', 413) ('an', 408) ('its', 388) ('about', 371) ('one', 351) ('you', 351) ('mr', 349) ('new', 349) ('also', 348) ('were', 344) ('mobile', 343) ('than', 343) ('we', 336) ('would', 322) ('been', 312) ('could', 308) ('technology', 303)\n",
      "\n",
      "politics --> ('the', 7957) ('to', 3913) ('of', 2840) ('and', 2559) ('in', 2159) ('said', 1445) ('he', 1410) ('for', 1237) ('that', 1195) ('on', 1186) ('is', 1167) ('be', 1080) ('mr', 1073) ('was', 1013) ('it', 998) ('would', 712) ('as', 690) ('have', 669) ('but', 668) ('not', 662) ('by', 651) ('with', 645) ('will', 642) ('are', 635) ('has', 611) ('they', 572) ('at', 564) ('his', 540) ('labour', 494) ('from', 466) ('government', 464) ('had', 462) ('an', 429) ('election', 424) ('we', 416) ('this', 410) ('blair', 395) ('were', 379) ('been', 377) ('party', 376) ('people', 372) ('there', 364) ('their', 357) ('which', 340) ('who', 330) ('also', 308) ('more', 298) ('if', 289) ('minister', 286) ('up', 284)\n",
      "\n",
      "sport --> ('the', 6620) ('to', 3189) ('and', 2532) ('in', 2510) ('of', 1826) ('for', 1127) ('he', 1105) ('on', 1014) ('but', 992) ('is', 985) ('it', 974) ('was', 943) ('that', 863) ('have', 812) ('with', 803) ('at', 794) ('his', 762) ('we', 660) ('has', 650) ('said', 636) ('be', 614) ('will', 575) ('as', 547) ('not', 490) ('from', 481) ('after', 477) ('by', 430) ('had', 414) ('they', 414) ('their', 381) ('been', 363) ('are', 356) ('game', 356) ('an', 353) ('this', 353) ('out', 351) ('first', 350) ('year', 331) ('england', 329) ('who', 324) ('against', 312) ('time', 296) ('when', 295) ('win', 295) ('up', 294) ('two', 290) ('world', 269) ('all', 268) ('over', 267) ('there', 264)\n",
      "\n",
      "entertainment --> ('the', 5822) ('and', 2130) ('to', 2108) ('of', 2048) ('in', 1996) ('for', 1097) ('on', 881) ('was', 818) ('it', 738) ('is', 684) ('with', 662) ('said', 594) ('he', 584) ('film', 583) ('at', 554) ('be', 502) ('that', 491) ('as', 479) ('has', 471) ('by', 460) ('his', 460) ('will', 433) ('best', 430) ('have', 384) ('who', 358) ('from', 356) ('are', 331) ('but', 326) ('year', 315) ('which', 305) ('an', 299) ('also', 277) ('been', 276) ('this', 266) ('one', 265) ('us', 264) ('had', 263) ('were', 262) ('music', 255) ('not', 251) ('they', 241) ('she', 240) ('we', 236) ('their', 235) ('new', 234) ('up', 225) ('show', 222) ('her', 215) ('first', 188) ('after', 186)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('English Dataset.csv')\n",
    "\n",
    "# since ArticleId has no effect to classify, we ignored this column\n",
    "\n",
    "dataset.drop('ArticleId', inplace=True, axis=1) \n",
    "dataset = dataset.reset_index(drop=True) \n",
    "\n",
    "# distinct categories\n",
    "categories = dataset['Category'].unique()\n",
    "\n",
    "\n",
    "a = dataset.to_numpy()\n",
    "print(a)\n",
    "\n",
    "cat_business = []\n",
    "cat_tech = []\n",
    "cat_politics = []\n",
    "cat_sports = []\n",
    "cat_entertainment = []\n",
    "\n",
    "\n",
    "# collecting whole texts of each category\n",
    "whole_category = [cat_business, cat_tech, cat_politics, cat_sports, cat_entertainment]\n",
    "\n",
    "for i in range(len(a)):\n",
    "    if a[i][1] == categories[0]:\n",
    "        cat_business.append(a[i][0])\n",
    "        \n",
    "    elif a[i][1] == categories[1]:\n",
    "        cat_tech.append(a[i][0])\n",
    "        \n",
    "    elif a[i][1] == categories[2]:\n",
    "        cat_politics.append(a[i][0])\n",
    "        \n",
    "    elif a[i][1] == categories[3]:\n",
    "        cat_sports.append(a[i][0])\n",
    "        \n",
    "    elif a[i][1] == categories[4]:\n",
    "        cat_entertainment.append(a[i][0])\n",
    "        \n",
    "from collections import defaultdict\n",
    "#         \n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "for c in range(len(whole_category)):\n",
    "    \n",
    "    cat_name = whole_category[c] \n",
    "    \n",
    "    # for each category we create a matrix\n",
    "    X = vectorizer.fit_transform(cat_name) \n",
    "    matrix = X.toarray()\n",
    "\n",
    "    # collecting columns of matrix which gives words of each category         \n",
    "    columns = (vectorizer.get_feature_names_out())\n",
    "    \n",
    "    # purpose of the summation of the columns is finding the word which has most frequency\n",
    "    sumOfColumns = np.sum(matrix, axis=0)\n",
    "\n",
    "    # keeping words and number of words in dict_word for each category\n",
    "    dict_word = defaultdict(int)\n",
    "    print(categories[c] , '-->', end=' ')\n",
    "    \n",
    "    # finding most frequent words for each category\n",
    "\n",
    "    for i in range(50):\n",
    "\n",
    "        max_value = np.max(sumOfColumns)\n",
    "\n",
    "        for j in range(len(sumOfColumns)):\n",
    "            if sumOfColumns[j] == max_value:\n",
    "                dict_word[columns[j]] = max_value\n",
    "                \n",
    "        # result means index of first max summation\n",
    "        result = np.where(sumOfColumns == max_value)[0][0]\n",
    "        # the reason behind assinging result to zero is that not considering the same max value\n",
    "        sumOfColumns[result] = 0\n",
    "    print(*dict_word.items())\n",
    "    \n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2e7b1",
   "metadata": {},
   "source": [
    "OUTPUT :\n",
    "\n",
    "business --> ('the', 7133) ('to', 3306) ('of', 2864) ('in', 2821) ('and', 2161) ('said', 1100) ('is', 1072) ('that', 1052) ('for', 1045) ('it', 1011) ('on', 906) ('has', 835) ('its', 736) ('by', 718) ('with', 612) ('at', 609) ('as', 605) ('was', 596) ('from', 588) ('be', 573) ('have', 554) ('are', 538) ('us', 522) ('will', 520) ('year', 456) ('which', 404) ('mr', 393) ('but', 391) ('an', 385) ('had', 343) ('this', 322) ('would', 309) ('been', 307) ('up', 306) ('not', 302) ('more', 298) ('he', 287) ('market', 284) ('were', 282) ('also', 279) ('than', 275) ('new', 273) ('their', 265) ('firm', 261) ('growth', 257) ('company', 253) ('last', 236) ('economy', 233) ('about', 228) ('after', 221)\n",
    "\n",
    "tech --> ('the', 7498) ('to', 4149) ('of', 3425) ('and', 3017) ('in', 2316) ('that', 1676) ('is', 1597) ('it', 1465) ('for', 1299) ('on', 1111) ('said', 1064) ('be', 1057) ('are', 994) ('as', 929) ('will', 797) ('with', 787) ('have', 731) ('by', 727) ('has', 686) ('was', 653) ('people', 647) ('they', 630) ('more', 624) ('not', 535) ('at', 533) ('but', 527) ('which', 514) ('from', 500) ('he', 484) ('can', 480) ('this', 474) ('or', 452) ('their', 422) ('up', 413) ('an', 408) ('its', 388) ('about', 371) ('one', 351) ('you', 351) ('mr', 349) ('new', 349) ('also', 348) ('were', 344) ('mobile', 343) ('than', 343) ('we', 336) ('would', 322) ('been', 312) ('could', 308) ('technology', 303)\n",
    "\n",
    "politics --> ('the', 7957) ('to', 3913) ('of', 2840) ('and', 2559) ('in', 2159) ('said', 1445) ('he', 1410) ('for', 1237) ('that', 1195) ('on', 1186) ('is', 1167) ('be', 1080) ('mr', 1073) ('was', 1013) ('it', 998) ('would', 712) ('as', 690) ('have', 669) ('but', 668) ('not', 662) ('by', 651) ('with', 645) ('will', 642) ('are', 635) ('has', 611) ('they', 572) ('at', 564) ('his', 540) ('labour', 494) ('from', 466) ('government', 464) ('had', 462) ('an', 429) ('election', 424) ('we', 416) ('this', 410) ('blair', 395) ('were', 379) ('been', 377) ('party', 376) ('people', 372) ('there', 364) ('their', 357) ('which', 340) ('who', 330) ('also', 308) ('more', 298) ('if', 289) ('minister', 286) ('up', 284)\n",
    "\n",
    "sport --> ('the', 6620) ('to', 3189) ('and', 2532) ('in', 2510) ('of', 1826) ('for', 1127) ('he', 1105) ('on', 1014) ('but', 992) ('is', 985) ('it', 974) ('was', 943) ('that', 863) ('have', 812) ('with', 803) ('at', 794) ('his', 762) ('we', 660) ('has', 650) ('said', 636) ('be', 614) ('will', 575) ('as', 547) ('not', 490) ('from', 481) ('after', 477) ('by', 430) ('had', 414) ('they', 414) ('their', 381) ('been', 363) ('are', 356) ('game', 356) ('an', 353) ('this', 353) ('out', 351) ('first', 350) ('year', 331) ('england', 329) ('who', 324) ('against', 312) ('time', 296) ('when', 295) ('win', 295) ('up', 294) ('two', 290) ('world', 269) ('all', 268) ('over', 267) ('there', 264)\n",
    "\n",
    "entertainment --> ('the', 5822) ('and', 2130) ('to', 2108) ('of', 2048) ('in', 1996) ('for', 1097) ('on', 881) ('was', 818) ('it', 738) ('is', 684) ('with', 662) ('said', 594) ('he', 584) ('film', 583) ('at', 554) ('be', 502) ('that', 491) ('as', 479) ('has', 471) ('by', 460) ('his', 460) ('will', 433) ('best', 430) ('have', 384) ('who', 358) ('from', 356) ('are', 331) ('but', 326) ('year', 315) ('which', 305) ('an', 299) ('also', 277) ('been', 276) ('this', 266) ('one', 265) ('us', 264) ('had', 263) ('were', 262) ('music', 255) ('not', 251) ('they', 241) ('she', 240) ('we', 236) ('their', 235) ('new', 234) ('up', 225) ('show', 222) ('her', 215) ('first', 188) ('after', 186)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f9814",
   "metadata": {},
   "source": [
    "most meaningful 3 words of each category:\n",
    "\n",
    "total words of business is 112281\n",
    "Business -->  market : 284, firm : 261, growth : 257\n",
    "\n",
    "total words of Tech is 130985\n",
    "Tech --> people : 647, mobile : 343 technology : 303\n",
    "\n",
    "total words of Politics is 123215   \n",
    "Politics --> labour : 494, government : 464, election : 424\n",
    "  \n",
    "total words of Sports is 116030\n",
    "Sports --> game : 356, first : 350, england : 329 \n",
    "\n",
    "total words of Entertainment is 91158\n",
    "Entertainment --> film : 583, best : 430, music : 255 \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f240b2",
   "metadata": {},
   "source": [
    "# Part 2: Implementing Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1795f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram:  99.32885906040268\n",
      "Bigram:  100.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset_array = dataset.to_numpy()\n",
    "\n",
    "np.random.shuffle(dataset_array) # shuffling dataset_array\n",
    "test = dataset_array[4 * len(dataset_array) // 5:].copy()\n",
    "training = dataset_array[:4 * len(dataset_array) // 5].copy()\n",
    "cat_business = []\n",
    "cat_tech = []\n",
    "cat_politics = []\n",
    "cat_sports = []\n",
    "cat_entertainment = []\n",
    "\n",
    "whole_category = [cat_business, cat_tech, cat_politics, cat_sports, cat_entertainment]\n",
    "\n",
    "# listing lengths of total words in every categories. \n",
    "length_of_words_categorically = [0,0,0,0,0]\n",
    "\n",
    "# collecting whole texts of each category in addition , finding total number of words of each category.\n",
    "\n",
    "\n",
    "for i in range(len(dataset_array)):\n",
    "    if dataset_array[i][1] == categories[0]:\n",
    "        cat_business.append(dataset_array[i][0])\n",
    "        length_of_words_categorically[0] += len(dataset_array[i][0].split())\n",
    "    elif dataset_array[i][1] == categories[1]:\n",
    "        cat_tech.append(dataset_array[i][0])\n",
    "        length_of_words_categorically[1] += len(dataset_array[i][0].split())\n",
    "    elif dataset_array[i][1] == categories[2]:\n",
    "        cat_politics.append(dataset_array[i][0])\n",
    "        length_of_words_categorically[2] += len(dataset_array[i][0].split())\n",
    "    elif dataset_array[i][1] == categories[3]:\n",
    "        cat_sports.append(dataset_array[i][0])\n",
    "        length_of_words_categorically[3] += len(dataset_array[i][0].split())\n",
    "    elif dataset_array[i][1] == categories[4]:\n",
    "        cat_entertainment.append(dataset_array[i][0])\n",
    "        length_of_words_categorically[4] += len(dataset_array[i][0].split())\n",
    "row = dataset.shape[0] # number of texts\n",
    "from collections import defaultdict\n",
    "vectorizer = CountVectorizer() # CountVectorizer for unigram matrix.\n",
    "vectorizer2 = CountVectorizer(ngram_range=(2, 2))# CountVectorizer for bigram matrix.\n",
    "whole_matrix = vectorizer.fit_transform(training[:, 0]).toarray()\n",
    "number_of_unique_words = whole_matrix.shape[1]\n",
    "whole_matrix_of_bigrams = vectorizer2.fit_transform(training[:, 0]).toarray()\n",
    "number_of_unique_bigrams = whole_matrix_of_bigrams.shape[1]\n",
    "bag_of_unigrams = defaultdict()\n",
    "bag_of_bigrams = defaultdict()\n",
    "# according to unigram and bigram matrices, creating dictionaries respectively\n",
    "for c in range(len(whole_category)):\n",
    "    cat_name = whole_category[c]\n",
    "    X = vectorizer.fit_transform(cat_name)\n",
    "    unigram_matrix = X.toarray()\n",
    "    words = (vectorizer.get_feature_names_out())\n",
    "    bigram_matrix = vectorizer2.fit_transform(cat_name).toarray()\n",
    "    bigrams = vectorizer2.get_feature_names_out()\n",
    "    vocabulary = defaultdict(int) # vocabulary of unigrams for each category\n",
    "    vocabulary2 = defaultdict(int) # vocabulary of bigrams for each category\n",
    "    sumOfColumns = np.sum(unigram_matrix, axis=0)\n",
    "    # adding existing words to unigram vocabulary\n",
    "    for i in range(unigram_matrix.shape[0]):\n",
    "        for j in range(unigram_matrix.shape[1]):\n",
    "            if unigram_matrix[i][j] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                vocabulary[words[j]] += unigram_matrix[i][j]\n",
    "    # collecting unigram vocabularies for each category\n",
    "    bag_of_unigrams[categories[c]] = vocabulary\n",
    "    # adding existing words to bigram vocabulary\n",
    "    for i in range(bigram_matrix.shape[0]):\n",
    "        for j in range(bigram_matrix.shape[1]):\n",
    "            if bigram_matrix[i][j] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                vocabulary2[bigrams[j]] += bigram_matrix[i][j]\n",
    "    # collecting bigram vocabularies for each category\n",
    "    bag_of_bigrams[categories[c]] = vocabulary2\n",
    "    \n",
    "def naive_bayes(unigrams,bigrams,test):\n",
    "    test = np.ndarray.tolist(test)\n",
    "    import re\n",
    "    global number_of_unique_words, length_of_words_categorically\n",
    "    for i in range(len(test)):\n",
    "        test[i] = test[i][0].lower() # converting every letter to lowercase \n",
    "        test[i] = re.sub(r'[^a-zA-Z0-9]', ' ', test[i]) # we are removing punctuations for not considering them as a word\n",
    "        test[i] = test[i].split() # converting text string to string list of every word.\n",
    "    predictions_unigram = []\n",
    "    predictions_bigram = []\n",
    "    for i in range(len(test)):\n",
    "        scores_unigram = [0,0,0,0,0] # keeping scores of each category for unigram\n",
    "        scores_bigram = [0,0,0,0,0] # keeping scores of each category for bigram\n",
    "        for j in range(len(test[i])):\n",
    "            for k in range(5):\n",
    "                if test[i][j] in unigrams[categories[k]].keys():\n",
    "                    scores_unigram[k] += np.log((unigrams[categories[k]][test[i][j]] +1) / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                else:\n",
    "                    scores_unigram[k] += np.log(1 / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                if j+2 < len(test[i]):\n",
    "                    bigram = test[i][j] + ' ' + test[i][j+1]\n",
    "                    if bigram in bigrams[categories[k]].keys():\n",
    "                        scores_bigram[k] += np.log((bigrams[categories[k]][bigram] + 1) / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                    else:\n",
    "                        scores_bigram[k] += np.log(1/(length_of_words_categorically[k]+number_of_unique_words))\n",
    "\n",
    "        predictions_unigram.append(categories[scores_unigram.index(max(scores_unigram))])\n",
    "        predictions_bigram.append(categories[scores_bigram.index(max(scores_bigram))])\n",
    "    return predictions_unigram , predictions_bigram\n",
    "\n",
    "def accuracy(outputs,predictions):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i] == predictions[i]:\n",
    "            correct_predictions +=1\n",
    "    return 100 * correct_predictions / len(outputs)\n",
    "\n",
    "predictions = naive_bayes(bag_of_unigrams,bag_of_bigrams,test)\n",
    "print(\"Unigram: \",accuracy(test[:,1],predictions[0]))\n",
    "print(\"Bigram: \",accuracy(test[:,1],predictions[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e2778",
   "metadata": {},
   "source": [
    "OUTPUT:\n",
    "    \n",
    "Unigram:  99.66442953020135\n",
    "\n",
    "Bigram:  100.0\n",
    "\n",
    "OUTPUT-2:\n",
    "\n",
    "Unigram:  98.99328859060402\n",
    "\n",
    "Bigram:  99.66442953020135\n",
    "\n",
    "After every running,\n",
    "implementation of naive bayes for bigram, classifies better than Unigram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75dcb7",
   "metadata": {},
   "source": [
    "# Part 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d381c",
   "metadata": {},
   "source": [
    "# Listing 10 words whose presence most strongly predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efb058",
   "metadata": {},
   "source": [
    "# First Perspective of Listing 10 words whose presence most strongly predicts\n",
    "\n",
    "We removed words if their idf is equal to 1, because meaning of idf is 1, the word exists in each text of that category so that word not too specific and strong. \n",
    "Combination of tf-idf and idf to get 10 most strongly predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "013e6c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business :  ['in', 'and', 'said', 'that', 'is', 'it', 'for', 'on', 'has', 'its']\n",
      "tech :  ['of', 'in', 'that', 'is', 'it', 'for', 'said', 'on', 'be', 'are']\n",
      "politics :  ['of', 'and', 'he', 'said', 'mr', 'for', 'on', 'that', 'is', 'be']\n",
      "sport :  ['to', 'in', 'and', 'of', 'he', 'is', 'we', 'for', 'it', 'on']\n",
      "entertainment :  ['to', 'and', 'in', 'of', 'for', 'film', 'on', 'was', 'it', 'he']\n"
     ]
    }
   ],
   "source": [
    "vectorizer3 = TfidfVectorizer()\n",
    "specific_word_for_each_category = []\n",
    "for c in range(len(whole_category)):\n",
    "    vectorizer3 = TfidfVectorizer()\n",
    "    X = vectorizer3.fit_transform(whole_category[c])\n",
    "    idfs = vectorizer3.idf_  \n",
    "\n",
    "    wordList = vectorizer3.get_feature_names_out()\n",
    "\n",
    "    tfidf_matrix = X.toarray()\n",
    "\n",
    "    specific_word = []\n",
    "    # to find the most frequent words \n",
    "    sumOfCol = np.sum(X.toarray(), axis=0)\n",
    "\n",
    "    for k in range(15):  # this range should be at least the length of the list (10)\n",
    "        max_val = np.max(sumOfCol)\n",
    "\n",
    "        for i in range(len(wordList)):\n",
    "            # meaning of idf is 1, the word exists in each text of that category. \n",
    "            # That's why we remove this word which is not too strong to predict the class \n",
    "            if idfs[i] == 1:\n",
    "                sumOfCol[i] = 0\n",
    "            if max_val != 0:\n",
    "                # after finding the most frequent word, adding to the list \n",
    "                # and update the index of sumOfCol for not considering it again \n",
    "                if sumOfCol[i] == max_val:\n",
    "                    specific_word.append(wordList[i])\n",
    "                    sumOfCol[i] = 0\n",
    "        # when the length of the list reached to 10, break the the loop\n",
    "        if len(specific_word) == 10: \n",
    "            break\n",
    "    specific_word_for_each_category.append(specific_word)\n",
    "    print(categories[c], \": \", specific_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc530e",
   "metadata": {},
   "source": [
    "Output of First Perspective of Listing 10 words whose presence most strongly predicts:\n",
    "\n",
    "business :  ['in', 'and', 'said', 'that', 'is', 'it', 'for', 'on', 'has', 'its']\n",
    "    \n",
    "tech :  ['of', 'in', 'that', 'is', 'it', 'for', 'said', 'on', 'be', 'are']\n",
    "    \n",
    "politics :  ['of', 'and', 'he', 'said', 'mr', 'for', 'on', 'that', 'is', 'be']\n",
    "    \n",
    "sport :  ['to', 'in', 'and', 'of', 'he', 'is', 'we', 'for', 'it', 'on']\n",
    "    \n",
    "entertainment :  ['to', 'and', 'in', 'of', 'for', 'film', 'on', 'was', 'it', 'he']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f99c0",
   "metadata": {},
   "source": [
    "# Second perspective of Listing 10 words whose presence most strongly predicts\n",
    "\n",
    "In this part we include words even if their idf is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9fec3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business :  ['the', 'to', 'in', 'of', 'and', 'said', 'that', 'is', 'it', 'for']\n",
      "tech :  ['the', 'to', 'of', 'and', 'in', 'that', 'is', 'it', 'for', 'said']\n",
      "politics :  ['the', 'to', 'of', 'and', 'in', 'he', 'said', 'mr', 'for', 'on']\n",
      "sport :  ['the', 'to', 'in', 'and', 'of', 'he', 'is', 'we', 'for', 'it']\n",
      "entertainment :  ['the', 'to', 'and', 'in', 'of', 'for', 'film', 'on', 'was', 'it']\n"
     ]
    }
   ],
   "source": [
    "vectorizer3 = TfidfVectorizer()\n",
    "specific_word_for_each_category = []\n",
    "for c in range(len(whole_category)):\n",
    "    vectorizer3 = TfidfVectorizer()\n",
    "    X = vectorizer3.fit_transform(whole_category[c])\n",
    "    idfs = vectorizer3.idf_  \n",
    "\n",
    "    wordList = vectorizer3.get_feature_names_out()\n",
    "\n",
    "    tfidf_matrix = X.toarray()\n",
    "\n",
    "    specific_word = []\n",
    "\n",
    "    sumOfCol = np.sum(X.toarray(), axis=0)\n",
    "\n",
    "    for k in range(15):  # this range should be at least the length of the list (10)\n",
    "        max_val = np.max(sumOfCol)\n",
    "\n",
    "        for i in range(len(wordList)):\n",
    "            \n",
    "            if max_val != 0:\n",
    "                if sumOfCol[i] == max_val:\n",
    "                    specific_word.append(wordList[i])\n",
    "                    sumOfCol[i] = 0\n",
    "\n",
    "        if len(specific_word) == 10:\n",
    "            break\n",
    "    specific_word_for_each_category.append(specific_word)\n",
    "    print(categories[c], \": \", specific_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54441406",
   "metadata": {},
   "source": [
    "Output Of Second perspective of Listing 10 words whose presence most strongly predicts:\n",
    "\n",
    "business :  ['the', 'to', 'in', 'of', 'and', 'said', 'that', 'is', 'it', 'for']\n",
    "    \n",
    "tech :  ['the', 'to', 'of', 'and', 'in', 'that', 'is', 'it', 'for', 'said']\n",
    "    \n",
    "politics :  ['the', 'to', 'of', 'and', 'in', 'he', 'said', 'mr', 'for', 'on']\n",
    "    \n",
    "sport :  ['the', 'to', 'in', 'and', 'of', 'he', 'is', 'we', 'for', 'it']\n",
    "    \n",
    "entertainment :  ['the', 'to', 'and', 'in', 'of', 'for', 'film', 'on', 'was', 'it']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78357b1c",
   "metadata": {},
   "source": [
    "# List the 10 words whose absence most strongly predicts \n",
    "\n",
    "Explaining logic of finding 10 words whose absence most strongly predicts is that:\n",
    "when idf of a word has max value, then this word appears rarely in the list \n",
    "and when sum of columns of tf-idf has min value, then it shows this word appearing rarely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c701965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business :  ['accessing', 'adequate', 'boundary', 'bracing', 'brighten', 'calculate', 'carefully', 'chairs', 'checked', 'circuits']\n",
      "tech :  ['amazed', 'astro', 'brits', 'cant', 'coffee', 'compass', 'diamond', 'divides', 'fizzy', 'froth']\n",
      "politics :  ['20p', '3rds', '3x', '5000', '50pc', '75p', '80s', 'absorb', 'adair', 'afloat']\n",
      "sport :  ['aggression', 'applaud', 'avoids', 'bangs', 'boil', 'buoyed', 'buts', 'characteristic', 'chases', 'cheering']\n",
      "entertainment :  ['11m', '42m', '60s', 'abating', 'admission', 'advances', 'advert', 'affection', 'airbrushes', 'amateurish']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "absence_word_for_each_category = []\n",
    "for c in range(len(whole_category)):\n",
    "    vectorizer3 = TfidfVectorizer()\n",
    "    Y = vectorizer3.fit_transform(whole_category[c])\n",
    "    idfs = vectorizer3.idf_ - 1\n",
    "\n",
    "    wordList = vectorizer3.get_feature_names_out()\n",
    "\n",
    "    tfidf_mat = Y.toarray()\n",
    "\n",
    "    sumOfCol = np.sum(Y.toarray(), axis=0)\n",
    "\n",
    "    max_idf = np.max(idfs)\n",
    "\n",
    "    min_sum = np.min(sumOfCol)\n",
    "\n",
    "    absence_word = []\n",
    "\n",
    "    for i in range(len(wordList)):\n",
    "        if idfs[i] == max_idf:\n",
    "            if sumOfCol[i] == min_sum:\n",
    "                absence_word.append(wordList[i])\n",
    "\n",
    "            if len(absence_word) == 10:\n",
    "                break\n",
    "    absence_word_for_each_category.append(absence_word)\n",
    "    print(categories[c], \": \", absence_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5204b9",
   "metadata": {},
   "source": [
    "Output of absecence list:\n",
    "\n",
    "business :  ['accessing', 'adequate', 'boundary', 'bracing', 'brighten', 'calculate', 'carefully', 'chairs', 'checked', 'circuits']\n",
    "    \n",
    "tech :  ['amazed', 'astro', 'brits', 'cant', 'coffee', 'compass', 'diamond', 'divides', 'fizzy', 'froth']\n",
    "    \n",
    "politics :  ['20p', '3rds', '3x', '5000', '50pc', '75p', '80s', 'absorb', 'adair', 'afloat']\n",
    "    \n",
    "sport :  ['aggression', 'applaud', 'avoids', 'bangs', 'boil', 'buoyed', 'buts', 'characteristic', 'chases', 'cheering']\n",
    "    \n",
    "entertainment :  ['11m', '42m', '60s', 'abating', 'admission', 'advances', 'advert', 'affection', 'airbrushes', 'amateurish']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016246d3",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "44b6cfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business :  ['year', 'mr', 'growth', 'sales', 'economy', 'market', 'bank', 'firm', 'oil', 'new']\n",
      "tech :  ['people', 'mobile', 'software', 'mr', 'phone', 'microsoft', 'music', 'games', 'net', 'users']\n",
      "politics :  ['mr', 'labour', 'election', 'blair', 'party', 'brown', 'government', 'people', 'tax', 'howard']\n",
      "sport :  ['england', 'game', 'year', 'world', 'win', 'time', 'wales', 'cup', 'ireland', 'chelsea']\n",
      "entertainment :  ['film', 'best', 'year', 'music', 'band', 'number', 'awards', 'new', 'award', 'won']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ENGLISH_STOP_WORDS = ENGLISH_STOP_WORDS.union(\n",
    "    [\"said\"])  # since every category has 'said', we carried 'said' to stop_word_list\n",
    "non_stop_specific_for_each_category = []\n",
    "for c in range(len(whole_category)):\n",
    "    vectorizer = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "    X = vectorizer.fit_transform(whole_category[c])\n",
    "    idfs = vectorizer.idf_ - 1\n",
    "\n",
    "    wordList = vectorizer.get_feature_names_out()\n",
    "\n",
    "    tfidf_matrix = X.toarray()\n",
    "\n",
    "\n",
    "    specific_word = []\n",
    "\n",
    "    sumOfCol = np.sum(X.toarray(), axis=0)\n",
    "\n",
    "    for k in range(22):  # this range should be at least the length of the list (10)\n",
    "        max_val = np.max(sumOfCol)\n",
    "\n",
    "        for i in range(len(wordList)):\n",
    "            if max_val != 0:\n",
    "                if sumOfCol[i] == max_val:\n",
    "                    specific_word.append(wordList[i])\n",
    "                    sumOfCol[i] = 0\n",
    "\n",
    "        if len(specific_word) == 10:\n",
    "            break\n",
    "    non_stop_specific_for_each_category.append(specific_word)\n",
    "    print(categories[c], \": \", specific_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5615a4",
   "metadata": {},
   "source": [
    "Output of non-stop words list:\n",
    "    \n",
    "business :  ['year', 'mr', 'growth', 'sales', 'economy', 'market', 'bank', 'firm', 'oil', 'new']\n",
    "    \n",
    "tech :  ['people', 'mobile', 'software', 'mr', 'phone', 'microsoft', 'music', 'games', 'net', 'users']\n",
    "    \n",
    "politics :  ['mr', 'labour', 'election', 'blair', 'party', 'brown', 'government', 'people', 'tax', 'howard']\n",
    "    \n",
    "sport :  ['england', 'game', 'year', 'world', 'win', 'time', 'wales', 'cup', 'ireland', 'chelsea']\n",
    "    \n",
    "entertainment :  ['film', 'best', 'year', 'music', 'band', 'number', 'awards', 'new', 'award', 'won']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e666b13",
   "metadata": {},
   "source": [
    "# Reimplementing naive bayes Algorithm for part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45080b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of part2 naive bayes unigram : 99.32885906040268\n",
      "Accuracy of part2 naive bayes bigram : 100.0\n",
      "Accuracy of part3 naive bayes unigram only using precences : 98.65771812080537\n",
      "Accuracy of part3 naive bayes bigram only using precences : 98.99328859060402\n",
      "Accuracy of part3 naive bayes unigram only using absences : 99.32885906040268\n",
      "Accuracy of part3 naive bayes bigram only using absences : 98.32214765100672\n",
      "Accuracy of part3 naive bayes unigram using both absences and presences : 98.65771812080537\n",
      "Accuracy of part3 naive bayes bigram using both absences and presences : 97.31543624161074\n",
      "Accuracy of part3 naive bayes unigram only using nonstop presences : 99.32885906040268\n",
      "Accuracy of part3 naive bayes bigram only using nonstop presences : 94.63087248322148\n"
     ]
    }
   ],
   "source": [
    "def improved_naive_bayes(unigrams,bigrams,test,specific_words,absence_words):\n",
    "    test = np.ndarray.tolist(test)\n",
    "    import re\n",
    "    global number_of_unique_words, length_of_words_categorically\n",
    "    for i in range(len(test)):\n",
    "        test[i] = test[i][0].lower()\n",
    "        test[i] = re.sub(r'[^a-zA-Z0-9]', ' ', test[i])\n",
    "        test[i] = test[i].split()\n",
    "    predictions_unigram = []\n",
    "    predictions_bigram = []\n",
    "    for i in range(len(test)):\n",
    "        scores_unigram = [0,0,0,0,0]\n",
    "        scores_bigram = [0,0,0,0,0]\n",
    "        for k in range(5):\n",
    "            for unique_word in set(test[i]):\n",
    "                if unique_word in specific_words[k]:\n",
    "                    # because when we look at the final scores of each text \n",
    "                    # for each category with bag of unigram\n",
    "                    # difference between min score and max score is in general \n",
    "                    # about half of the length of text\n",
    "                    # and in general every presence word is in text. So , this will work 10 times\n",
    "                    #  so if an absence word of a class is in a text\n",
    "                    # we are decreasing the probability of the class but we are not doing it impossible.\n",
    "                    scores_unigram[k] += len(test[i]) * 0.05\n",
    "                    # because when we look at the final scores of each text \n",
    "                    # for each category with bag of bigram\n",
    "                    # difference between min score and max score is in general \n",
    "                    # about 7 times of the length of text\n",
    "                    # and in general every presence word is in text. So , this will work 10 times\n",
    "                    # so if an absence word of a class is in a text\n",
    "                    # we are decreasing the probability of the class but we are not doing it impossible.\n",
    "                    scores_bigram[k] += len(test[i]) * 0.35\n",
    "            for j in range(len(test[i])):\n",
    "                if test[i][j] in absence_words[k]:\n",
    "                    # because when we look at the final scores of each text \n",
    "                    # for each category with bag of unigram\n",
    "                    # difference between min score and max score is in general about half of\n",
    "                    # the length of text so if an absence word of a class is in a text\n",
    "                    # we are decreasing the probability of the class but we are not doing it impossible.\n",
    "                    scores_unigram[k] -= len(test[i]) * 0.5\n",
    "                    # because when we look at the final scores of each text \n",
    "                    # for each category with bag of bigram\n",
    "                    # difference between min score and max score is in general \n",
    "                    # about 7 times of the length of text\n",
    "                    # so if an absence word of a class is in a text\n",
    "                    # we are decreasing the probability of the class but we are not doing it impossible.\n",
    "                    scores_bigram[k] -= len(test[i]) * 3.5\n",
    "                if test[i][j] in unigrams[categories[k]].keys():\n",
    "                    scores_unigram[k] += np.log((unigrams[categories[k]][test[i][j]] +1) / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                else:\n",
    "                    scores_unigram[k] += np.log(1 / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                if j+2 < len(test[i]):\n",
    "                    bigram = test[i][j] + ' ' + test[i][j+1]\n",
    "                    if bigram in bigrams[categories[k]].keys():\n",
    "                        scores_bigram[k] += np.log((bigrams[categories[k]][bigram] + 1) / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                    else:\n",
    "                        scores_bigram[k] += np.log(1/(length_of_words_categorically[k]+number_of_unique_words))\n",
    "        predictions_unigram.append(categories[scores_unigram.index(max(scores_unigram))])\n",
    "        predictions_bigram.append(categories[scores_bigram.index(max(scores_bigram))])\n",
    "    return predictions_unigram , predictions_bigram\n",
    "\n",
    "predictions = naive_bayes(bag_of_unigrams,bag_of_bigrams,test)\n",
    "print(\"Accuracy of part2 naive bayes unigram :\" ,accuracy(test[:,1],predictions[0]))\n",
    "print(\"Accuracy of part2 naive bayes bigram :\" ,accuracy(test[:,1],predictions[1]))\n",
    "predictions2 = improved_naive_bayes(bag_of_unigrams,bag_of_bigrams,test,[[],[],[],[],[]],absence_word_for_each_category)\n",
    "print(\"Accuracy of part3 naive bayes unigram only using precences :\" ,accuracy(test[:,1],predictions2[0]))\n",
    "print(\"Accuracy of part3 naive bayes bigram only using precences :\" ,accuracy(test[:,1],predictions2[1]))\n",
    "predictions3 = improved_naive_bayes(bag_of_unigrams,bag_of_bigrams,test,specific_word_for_each_category,[[],[],[],[],[]])\n",
    "print(\"Accuracy of part3 naive bayes unigram only using absences :\" ,accuracy(test[:,1],predictions3[0]))\n",
    "print(\"Accuracy of part3 naive bayes bigram only using absences :\" ,accuracy(test[:,1],predictions3[1]))\n",
    "predictions4 = improved_naive_bayes(bag_of_unigrams,bag_of_bigrams,test,specific_word_for_each_category,absence_word_for_each_category)\n",
    "print(\"Accuracy of part3 naive bayes unigram using both absences and presences :\" ,accuracy(test[:,1],predictions4[0]))\n",
    "print(\"Accuracy of part3 naive bayes bigram using both absences and presences :\" ,accuracy(test[:,1],predictions4[1]))\n",
    "predictions5 = improved_naive_bayes(bag_of_unigrams,bag_of_bigrams,test,non_stop_specific_for_each_category,[[],[],[],[],[]])\n",
    "print(\"Accuracy of part3 naive bayes unigram only using nonstop presences :\" ,accuracy(test[:,1],predictions5[0]))\n",
    "print(\"Accuracy of part3 naive bayes bigram only using nonstop presences :\" ,accuracy(test[:,1],predictions5[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c51ee9",
   "metadata": {},
   "source": [
    "# Output 1:\n",
    "\n",
    "Accuracy of part2 naive bayes unigram : 98.99328859060402\n",
    "\n",
    "Accuracy of part2 naive bayes bigram : 99.66442953020135\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using precences : 98.99328859060402\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using precences : 99.32885906040268\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using absences : 98.99328859060402\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using absences : 96.97986577181209\n",
    "\n",
    "Accuracy of part3 naive bayes unigram using both absences and presences : 98.99328859060402\n",
    "\n",
    "Accuracy of part3 naive bayes bigram using both absences and presences : 96.64429530201342\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using nonstop presences : 98.65771812080537\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using nonstop presences : 95.63758389261746\n",
    "\n",
    "# Output 2:\n",
    "\n",
    "Accuracy of part2 naive bayes unigram : 100.0\n",
    "\n",
    "Accuracy of part2 naive bayes bigram : 100.0\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using precences : 100.0\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using precences : 100.0\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using absences : 100.0\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using absences : 97.31543624161074\n",
    "\n",
    "Accuracy of part3 naive bayes unigram using both absences and presences : 100.0\n",
    "\n",
    "Accuracy of part3 naive bayes bigram using both absences and presences : 97.31543624161074\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using nonstop presences : 99.66442953020135\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using nonstop presences : 95.30201342281879\n",
    "\n",
    "# Output 3\n",
    "\n",
    "Accuracy of part2 naive bayes unigram : 99.32885906040268\n",
    "\n",
    "Accuracy of part2 naive bayes bigram : 100.0\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using precences : 98.65771812080537\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using precences : 98.99328859060402\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using absences : 99.32885906040268\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using absences : 98.32214765100672\n",
    "\n",
    "Accuracy of part3 naive bayes unigram using both absences and presences : 98.65771812080537\n",
    "\n",
    "Accuracy of part3 naive bayes bigram using both absences and presences : 97.31543624161074\n",
    "\n",
    "Accuracy of part3 naive bayes unigram only using nonstop presences : 99.32885906040268\n",
    "\n",
    "Accuracy of part3 naive bayes bigram only using nonstop presences : 94.63087248322148"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733900da",
   "metadata": {},
   "source": [
    "# Effect of removing stop-words when improving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d2d84bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram:  99.66442953020135\n",
      "Bigram:  65.43624161073825\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS) # CountVectorizer for unigram matrix.\n",
    "vectorizer2 = CountVectorizer(ngram_range=(2, 2),stop_words=ENGLISH_STOP_WORDS)# CountVectorizer for bigram matrix.\n",
    "whole_matrix = vectorizer.fit_transform(training[:, 0]).toarray()\n",
    "number_of_unique_words = whole_matrix.shape[1]\n",
    "whole_matrix_of_bigrams = vectorizer2.fit_transform(training[:, 0]).toarray()\n",
    "number_of_unique_bigrams = whole_matrix_of_bigrams.shape[1]\n",
    "bag_of_unigrams = defaultdict()\n",
    "bag_of_bigrams = defaultdict()\n",
    "# according to unigram and bigram matrices, creating dictionaries respectively\n",
    "for c in range(len(whole_category)):\n",
    "    cat_name = whole_category[c]\n",
    "    X = vectorizer.fit_transform(cat_name)\n",
    "    unigram_matrix = X.toarray()\n",
    "    words = (vectorizer.get_feature_names_out())\n",
    "    bigram_matrix = vectorizer2.fit_transform(cat_name).toarray()\n",
    "    bigrams = vectorizer2.get_feature_names_out()\n",
    "    vocabulary = defaultdict(int) # vocabulary of unigrams for each category\n",
    "    vocabulary2 = defaultdict(int) # vocabulary of bigrams for each category\n",
    "    sumOfColumns = np.sum(unigram_matrix, axis=0)\n",
    "    # adding existing words to unigram vocabulary\n",
    "    for i in range(unigram_matrix.shape[0]):\n",
    "        for j in range(unigram_matrix.shape[1]):\n",
    "            if unigram_matrix[i][j] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                vocabulary[words[j]] += unigram_matrix[i][j]\n",
    "    # collecting unigram vocabularies for each category\n",
    "    bag_of_unigrams[categories[c]] = vocabulary\n",
    "    # adding existing words to bigram vocabulary\n",
    "    for i in range(bigram_matrix.shape[0]):\n",
    "        for j in range(bigram_matrix.shape[1]):\n",
    "            if bigram_matrix[i][j] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                vocabulary2[bigrams[j]] += bigram_matrix[i][j]\n",
    "    # collecting bigram vocabularies for each category\n",
    "    bag_of_bigrams[categories[c]] = vocabulary2\n",
    "    \n",
    "def naive_bayes(unigrams,bigrams,test):\n",
    "    test = np.ndarray.tolist(test)\n",
    "    import re\n",
    "    global number_of_unique_words, length_of_words_categorically\n",
    "    for i in range(len(test)):\n",
    "        test[i] = test[i][0].lower() # converting every letter to lowercase \n",
    "        test[i] = re.sub(r'[^a-zA-Z0-9]', ' ', test[i]) # we are removing punctuations for not considering them as a word\n",
    "        test[i] = test[i].split() # converting text string to string list of every word.\n",
    "    predictions_unigram = []\n",
    "    predictions_bigram = []\n",
    "    for i in range(len(test)):\n",
    "        scores_unigram = [0,0,0,0,0] # keeping scores of each category for unigram\n",
    "        scores_bigram = [0,0,0,0,0] # keeping scores of each category for bigram\n",
    "        for j in range(len(test[i])):\n",
    "            for k in range(5):\n",
    "                if test[i][j] in unigrams[categories[k]].keys():\n",
    "                    scores_unigram[k] += np.log((unigrams[categories[k]][test[i][j]] +1) / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                else:\n",
    "                    scores_unigram[k] += np.log(1 / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                if j+2 < len(test[i]):\n",
    "                    bigram = test[i][j] + ' ' + test[i][j+1]\n",
    "                    if bigram in bigrams[categories[k]].keys():\n",
    "                        scores_bigram[k] += np.log((bigrams[categories[k]][bigram] + 1) / (length_of_words_categorically[k] + number_of_unique_words))\n",
    "                    else:\n",
    "                        scores_bigram[k] += np.log(1/(length_of_words_categorically[k]+number_of_unique_words))\n",
    "\n",
    "        predictions_unigram.append(categories[scores_unigram.index(max(scores_unigram))])\n",
    "        predictions_bigram.append(categories[scores_bigram.index(max(scores_bigram))])\n",
    "    return predictions_unigram , predictions_bigram\n",
    "\n",
    "def accuracy(outputs,predictions):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i] == predictions[i]:\n",
    "            correct_predictions +=1\n",
    "    return 100 * correct_predictions / len(outputs)\n",
    "\n",
    "predictions = naive_bayes(bag_of_unigrams,bag_of_bigrams,test)\n",
    "print(\"Unigram: \",accuracy(test[:,1],predictions[0]))\n",
    "print(\"Bigram: \",accuracy(test[:,1],predictions[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603cf65",
   "metadata": {},
   "source": [
    "Output 1:\n",
    "\n",
    "Unigram:  99.66442953020135\n",
    "    \n",
    "Bigram:  65.43624161073825\n",
    "    \n",
    "Output 2:\n",
    "\n",
    "Unigram:  99.66442953020135\n",
    "    \n",
    "Bigram:  69.79865771812081\n",
    "    \n",
    "Output 3:\n",
    "\n",
    "Unigram:  99.32885906040268\n",
    "    \n",
    "Bigram:  71.81208053691275"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a756e6",
   "metadata": {},
   "source": [
    "The reason behind decreasing accuracy rates of bigram is that when we remove stopwords \n",
    "from the entire dataset, unrelated word pairs will increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265aae08",
   "metadata": {},
   "source": [
    "# Part 4 accuracy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33425c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs,predictions):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i] == predictions[i]:\n",
    "            correct_predictions +=1\n",
    "    return 100 * correct_predictions / len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfeaaae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
